[{"categories":["Productivity"],"content":"Productivity So you probably have a couple thousand and a half things to do and 99% of them should have been done by now, but you didn\u0026rsquo;t get enough time and that\u0026rsquo;s why you will start tomorrow fresh again on achieving these, where tomorrow is some mystical land where 99% of the human productivity lives. Do you see a common pattern over the years where you wanted to do a lot of things but weren\u0026rsquo;t able to? Yes there probably are a lot of things that were done wrong, and you might need to even start to rethink from scratch not just when would you achieve your goals or when your tomorrow or monday will come, but rather rationalise your thinking about goals and productivity. This page will contain some of the things that I do or I learnt the hard way.\nYou own all your time What you are doing at the moment is the best use of your time according to you. Stop saying I don\u0026rsquo;t have time rather say it\u0026rsquo;s not my priority and then reflect in your subconscious head Is what I am doing right now really my priority over the thing I am choosing not to do? But have a conviction either you are going to do or you are not, don\u0026rsquo;t stay in between. This is one of my favourite tips which genuinely lets me take more control over my time.\nUse a to-do list Are you a software developer? Are you a product manager? Do you have a list of what needs to be done in the quarter or in a sprint for the company that you are working for? You most probably do. You can\u0026rsquo;t even imagine starting a sprint without having a proper to-do list for the sprint. Then why don\u0026rsquo;t you have a to-do list for your life outside of the company you work for? But I have all of this in my head(whatever I need to do)- Our brain is unfortunately optimized for having ideas. not storing them. The idea is inspired from Getting Things Done by David Allen. Have an inbox accessible to you most of the time, whenever you have an idea or a todo task, add it in your inbox. Delegate a project to it, and it should vanish from your inbox. Keep delegating your todos from inbox to projects to make space for new todos. I usually assign them labels which usually falls in one of these categories [\u0026quot;high-energy\u0026quot;,\u0026quot;low-energy\u0026quot;,\u0026quot;bed\u0026quot;,\u0026quot;mobile\u0026quot;,\u0026quot;desk\u0026quot;] and I can look at my todo list and grab the high-energy,desk tasks in the morning and low-energy tasks in the evening and make sure they are completed. I have a recurring task- plan out tomorrow where I need to look at my projects or the jira for my company and plan out what my tomorrow would look like. I have another recurring task retrospect today where I need to look at how my day went. A simple todoist app works too, no need to go fancy and build custom dashboards for GTD in notion or anywhere else. Just make sure, everything goes in the todo list. If watching netflix for 5 hours is what you are going to do tomorrow, put it up in the todo list, but if not that should be the first thing that goes out of that list.\nUse a time tracker tool Use a time tracker tool better something that integrates with your todo tool and track your time spend on a task. I use clockify which integrates nicely with todoist and I can in one click start a clockify timer from either my chrome extension or inside the todoist app and it starts tracking time on my calendar and gives a nice visualization of where my time went on the calendar! But wait do I have to tick the timer for each meeting too? I already took the pain to scheduled it. No worries, clockify synchronizes nicely with google calendar too, and you can connect your calendar to have your events on the clockify calendar. But I schedule meetings on my official calendar, and that\u0026rsquo;s not permanent. What do you have for that? I use a tool called zapier which copies the events to my personal calendar in realtime- as soon as a new event is triggered in my official calendar and that\u0026rsquo;s how I am always on top of my time.\nReflect on your goals seldom Always fell in love with the process, but no matter how beautiful the process is- seldom take out time to reflect on the goals. Since productivity isn\u0026rsquo;t about how much busywork you could cram in an hour, how organized you make your life, how beautiful your calendar looks, how everything is measured or how you have everything scheduled by a fancy tool/app. It\u0026rsquo;s about the results you product, if you\u0026rsquo;re not producing results in the things that matter, you\u0026rsquo;re not getting anymore productive, you are just doing more busy work more efficiently. Ultimately we would like to be able to get the things done, we never could.\nAllocate the adequate time to the work If you alot yourself a day to write a simple unit test, it would probably take all that much. This is the idea of parkinson\u0026rsquo;s law which states that work expands to fill the time that we allocate to it. The idea is to keep short deadlines for the tasks, sometimes even unrealistic deadlines would work too. You\u0026rsquo;ll not achieve 100% of the goal, but you know anyways that was an unrealistic deadline but that deadline would help you get way ahead in the progress. This I have learnt a hard way- if I don\u0026rsquo;t have anything planned out for the day, and I just have to write a unit test which should probably take not more than 10 minutes, I end up writing that unit test only in the entire day even though I was on my laptop the whole day. I just keep on beautifying it, make sure the table driven tests that I have written has the nicest table structure etc.\n2-minute, 10-minute, 30-minute rule Make a rule that if a task takes anything less than 2 minutes to complete, I will just complete it straight away rather than queuing it up in a pile and then dequeuing it when it is no longer even substantial. For example- I need to track the interview that I give on huntr or I just need to jot down the points for the rejection email that I just received, it is probably going to take somewhere in the 2 minutes range window, therefore I will just do it straightaway rather than stacking it down to the pile of tasks. The 10 minute rule states- if you are ever feeling unproductive or demotivated or are running away from your task, just say to yourself I am just going to do it whole heartedly for 10 minutes nothing less nothing more. Once you hit somewhere around 10 mintues mark, you\u0026rsquo;d have probably already developed interest in doing it and you can keep doing it further too. 30-minute rule is also known as pomodoro rule which is a chinese technique (here 30 minute is in no sense a hard limit as with the above 2 as well). So whatever work you are doing, start a timer, disable all distractions and start working until the timer ticks. It is like a sprint, and when you are sprinting you are just sprinting with your whole heart towards the track(here your task). No matter what don\u0026rsquo;t get distracted for those few minutes where you are sprinting to get your work done. Minus to disable some notifications and minimize distractions via popups.\nDelegate You are probably saying yes to everything in your inbox, do those things have to be done specifically by you? I use a simple Eisenhower Decision Marix to decide whether it can be delegated or not or when do I need to pick it up.\nRefactor your resources- don\u0026rsquo;t read thousands of blogs, have some trusted sources where you would information about what\u0026rsquo;s happening in the world, what is the state of art technology or whatever is related to your job. Having a refactored inbox is a must- group all the CCs into a different inbox folder, don\u0026rsquo;t religiously read and act upon CC\u0026rsquo;d emails to you, and next time your boss asks you about a status of work that he CC\u0026rsquo;d you on, just tell him I thought you were informing me and not assigning the work to me. Please next time send a direct email to me for the work I need to get done. Have a strong spam filtering so that an ad for a commodity doesn\u0026rsquo;t make it to your inbox.\nAutomate the third time Always opt for the easier method between the automating task or just getting it done for the first couple of times, but once you need to do it for the third time, please invest some time wondering if it can be automated, how much time is it going to save me? Is it worth automating it? I have seen almost 100% of the time it is worth automating the stuff you do in your daily work. Optimize for shorter commands or shortcuts that you use very often. I have more than 1100 aliases and more than a dozen shell functions to make my life easier.\nThe next section will just contain the tools I use daily and would recommend everyone-\nxbar - this app, I last checked was specifically for mac and lets you put anything on your menu bar for mac. I have the PRs I have to review handy from anywhere and can go directly to that PR in one click, sometimes I do kprtfwd auth iden 8080 but don\u0026rsquo;t remember which kubecontext is currently active, here I go, I can just look at the menu bar no matter what foreground application I am running, and be sure about the kubecontext. Finicky - this allows for customized rules for browsing particular endpoints on a spceific browser. Since, I have 3 github accounts, it makes my life easy to choose finicky as the default browser and let it decide on which browser shall it open the endpoint. Yabai - A tiling manager for mac. Warp - terminal for Mac. Other options are- iTerm2, kitty,alakritty, hyper. I have used all of them and any of them were absolutely amazing. Todoist - to track my todos Huntr - to track my job applications, where I have applied, where I have got the offer letters from etc. Alfred - efficiency app for mac, replacement for spotlight searching. Github Desktop - desktop client for github, I personally didn\u0026rsquo;t find it useful after I had to contribute to 3 different github accounts, rather gh-cli is what I use more. Obsidian - notes taking app, with the amount of extensions and integrations available with obsidian, this notes taking app is simply amazing. Another alternatives are- notion, mem, one drive etc. Clockify - to track my time. BloomRPC - GUI to make gRPC calls easier. Postman - GUI for HTTP rest APIs and somewhat API documentation. reclaim - to reclaim my time and have some focus work time, so that colleagues don\u0026rsquo;t schedule a meeting in my claimed time until it\u0026rsquo;s p0. cron - a smart calendar app. pgAdmin4 - Postgres CLI Apps that might be good to have but doesn\u0026rsquo;t aid much to productivity.\nPock - widget manager app for mac touchbar. APowerMirror - mirror your phone on your mac, I don\u0026rsquo;t write mobile applications that\u0026rsquo;s why not very useful to me, but is very feature rich and reliant. Some CLI tools that I use and would recommend everyone-\nprotoc - for code generating client and API documentation using protos. github cli - to create issues directly from the CLi, create PRs etc. br - a replacement for tree exa - a replacement for ls eva - REPL calculator delta - diff tool zsh - shell, some zsh plugins that I use- git, docker, zsh-autosuggestions, fzf oh-my-zsh - configuration management for zsh iredis - intuitive redis CLI with auto completion pgcli - intuitive psql CLI with auto completion procs - replacement for ps Some useful shell functions-\nextract , no need to remember the command to extract the specific file type. 1extract () { 2 if [ -f $1 ] ; then 3 case $1 in 4 *.tar.bz2) tar xjf $1 ;; 5 *.tar.gz) tar xzf $1 ;; 6 *.bz2) bunzip2 $1 ;; 7 *.rar) unrar x $1 ;; 8 *.gz) gunzip $1 ;; 9 *.tar) tar xf $1 ;; 10 *.tar.xz) tar xf $1 ;; 11 *.tbz2) tar xjf $1 ;; 12 *.tgz) tar xzf $1 ;; 13 *.zip) unzip $1 ;; 14 *.Z) uncompress $1 ;; 15 *.7z) 7zr e $1 ;; 16 *) echo \u0026#34;\u0026#39;$1\u0026#39; cannot be extracted via extract()\u0026#34; ;; 17 esac 18 else 19 echo \u0026#34;\u0026#39;$1\u0026#39; is not a valid file\u0026#34; 20 fi 21} uuid- to get a Universally Unique Identifier, this is very useful to me when I need to create some records with some uuid manually for testing, so have it handy, for this you need to have jq installed. The source code is encoded in --data-raw flag. You can find here for easier read- https://go.dev/play/p/9Nbnd3glsp_g . You can pipe it into pbcopy to copy it onto clipboard. 1uuid(){ 2 curl \u0026#39;https://go.dev/_/compile?backend=\u0026#39; -H \u0026#39;accept: application/json, text/javascript, */*; q=0.01\u0026#39; -H \u0026#39;content-type: application/x-www-form-urlencoded; charset=UTF-8\u0026#39; --data-raw \u0026#39;version=2\u0026amp;body=package+main%0A%0Aimport+(%0A%09%22fmt%22%0A%0A%09%22github.com%2Fgoogle%2Fuuid%22%0A)%0A%0Afunc+main()+%7B%0A%09fmt.Print(uuid.New().String())%0A%7D%0A\u0026amp;withVet=true\u0026#39; --compressed | jq \u0026#34;.Events[0].Message\u0026#34; 3} dip- print out ips of all running docker containers 1function dip { 2 echo \u0026#34;IP addresses of all named running containers\u0026#34; 3 4 for DOC in `dnames-fn` 5 do 6 IP=`docker inspect --format=\u0026#39;{{range .NetworkSettings.Networks}}{{.IPAddress}} {{end}}\u0026#39; \u0026#34;$DOC\u0026#34;` 7 OUT+=$DOC\u0026#39;\\t\u0026#39;$IP\u0026#39;\\n\u0026#39; 8 done 9 echo -e $OUT | column -t 10 unset OUT 11} wp- watch pods, watch pods for specific app label in case of deployment if they are crashing or not etc. 1function wp { 2 watch -n1 \u0026#34;kubectl get pods -l app=$1 -n $2\u0026#34; 3} pods- get all pods in a specific namespace or in all namespaces, same can be done for other kubernetes resources too- for example- deployment, services, ingresses etc. 1function pods(){ 2 if [ $# -eq 0 ]; then 3 kubectl get pods -A | while read -r line; do echo \u0026#34;${line} | font=Menlo\u0026#34;; done 4 else 5 kubectl get pods -n $1 | while read -r line; do echo \u0026#34;${line} | font=Menlo\u0026#34;; done 6 fi 7} numPods- numPods just returns the number for the pods, same can be done with other kubernetes resources 1function numPods(){ 2 if [ $# -eq 0 ]; then 3 kubectl get pods -A 2\u0026gt; /dev/null | grep -v NAME | wc -l | sed \u0026#39;s/ //g\u0026#39; 4 else 5 kubectl get pods -n $1 2\u0026gt; /dev/null | grep -v NAME | wc -l | sed \u0026#39;s/ //g\u0026#39; 6 fi 7} kprtfwd can be used to port-forward a local port to service\u0026rsquo;s port. Most of my services run on 8080, therefore specifying 8080 each time is redundant, therefore if there are just 3 args, the default port is 8080, change it to your default port, but in some cases when the service is not running on 8080 and some other port, you can specify an extra argument. 1func kprtfwd(){ 2 if [ $# -eq 4 ]; then 3 kubectl port-forward -n $1 svc/$2 $3:$4 4 elif [ $# -eq 3 ]; then 5 kubectl port-forward -n $1 svc/$2 $3:8080 6 else 7 echo \u0026#34;usage: kprtfwd \u0026lt;namespace\u0026gt; \u0026lt;service\u0026gt; \u0026lt;port-on-local\u0026gt; \u0026lt;port-on-container(by default 8080 if not given)\u0026gt;\u0026#34; 8 fi 9} ","date":"Feb 12, 2022","img":"","permalink":"/posts/productivity/","series":["Personal Productivity"],"tags":["Leadership","productivity"],"title":"Productivity"},{"categories":["technical","engineering"],"content":"I heard you wanted a practical distributed systems, so here it is writing hello world of distributed systems - a distributed counter. For a little of background, let\u0026rsquo;s just say you are trying to write a likes service which tracks likes for a particular post and you want to horizontally scale this service by adding multiple instances of this service due to whatever reasons. So, let\u0026rsquo;s just talk about the ways to achieve this.\nThis seems like-\nAtleast once Exactly once Atmost once Where a single like by a user should be reflected exactly once for the post.\nUse a map of post to counter. Let\u0026rsquo;s just say the posts are unique based on their id and the id is represented in string, so an in memory database for that might look like map[string]int64. Now for each like you increase the counter for that post. Now all sorts of problem might arise and you need some sort of deduplication on requests which is not very easy to achieve. For example- you might have sent a post request to like a particular post and the request got through to the server but didn\u0026rsquo;t get through till the client and therefore you issue a request again, and so now you have got two likes for a user which has intended to like just once. So, you need to deduplicate the request here.\nNow let\u0026rsquo;s just say even the request got through to the server and the response got through as well, but while letting other instances know that you have received a like you might end up in the same scenario and then all sorts of inconsistencies arise across your service instances because of this. But you want consistency across the service instances, they might be very strongly consistent guaranteeing linearizability but you might still want eventual consistency.\nSo, what can be done?\nConsistency Without Consensus You want consistency in distributed systems, what are the options?\nConsensus- Very expensive 2PC Writes via leader- Very hard to scale, implicits consensus on scaling. CRDT- Consistency(eventual consistency) without consensus. Avoiding consensus as much as possible is the key to write distributed systems and systems that can scale horizontally without biting you.\nWhat\u0026rsquo;s CRDT now? CRDTs are conflict free replicated data types particularly used in distributed systems. Particularly referred to as ACID 2.0 types. Well, that\u0026rsquo;s not even a thing, but i like how somewhere CRDTs are reference like this and so i stole it.\nAcronym Meaning A Associative C Commutative I Idempotent D Distributed The D is not even a thing, just call it whatever you want, so we will call it distributed. The objective is to bring out consistency in distributed systems without consensus, as consensus is very costly and therefore we read about CRDTs. The main idea is to make the operations commute, idempotent and associative. So you start any number of replicas at any arbitrary state(same for the replicas) and apply operations in any order, and as they are associative, commutative and idempotent. You are sure to get the exact same state (after applying all the operations), even if the operations are not applied in the same order for both the nodes. This means the nodes that have seen same set of operations in any order are consistent with each other.\nCRDTs is a huge topic and out of scope of this article to completely cover that, but you can find various resources on CRDT online. You can find various CRDTs implemented here . I created this repo to play around with CRDTs for fun and is written in go. The specific CRDT that we are going to use right now for this particular case is G-set CRDT. G-set or Grow only set is a special type of CRDT set that allows only to add data to the set, and not remove it. So, you might have guessed already the user is allowed to like, but not unlike or dislike it.\nNow, let\u0026rsquo;s get to actual writing of the service. You can find the whole code here .\nBut let\u0026rsquo;s start in pieces on how to approach writing this?\nWe need some sort of database to store the likes for a post. We are going to use in memory database for this which is going to be a map[string]gset.Gset which a map of string to a gset. Going to use zerolog library for logging. We need someway to make a cluster of nodes for which we are going to use memberlist from hashicorp which is a hightly available cluster aware membership protocol and going to gossip the likes with each other and preiodically reduce the entropy among the nodes.\nTo subscribe to the gossip layer of the memberlist protocol you have to implement membership.Delegate interface. The NotifyMsg(b []byte) method of the interface is going to notify you of any incoming gossip message. On invoke of this method we send it in a channel for which a state machine is looping indefinitely until the channel is closed and then taking necessary actions as approrpiate on receive of a gossip message. The relevant code is-\n1func (s *server) NotifyMsg(b []byte) { 2 s.chanMsg \u0026lt;- b 3} and looping on this channel to listen for events-\n1func (s *server) loopSM() { 2 for i := range s.chanMsg { 3 s.logger.Debug().Msgf(\u0026#34;Received message %s\u0026#34;, string(i)) 4 var l likeDomain 5 err := json.Unmarshal(i, \u0026amp;l) 6 if err != nil { 7 s.logger.Debug().Msgf(\u0026#34;Error unmarshalling message %e\u0026#34;, err) 8 continue 9 } 10 // if error is not nil add that to gset. 11 _ = s.postLikes.AddLike(l.User, l.Post) 12 } 13} Also to reduce the entropy periodically we need to tell some local state of the node in some way which we achieve by\n1s.mu.Lock() 2defer s.mu.Unlock() 3b, _ := json.Marshal(s.postLikes) 4return b and then compare remote state received from some node\u0026rsquo;s global state.\n1func (s *server) MergeRemoteState(buf []byte, join bool) { 2 if len(buf) == 0 || !join { 3 return 4 } 5 var temp = map[string]gset.Gset{} 6 _ = json.Unmarshal(buf, \u0026amp;temp) // deliberately ignore error so linter doesn\u0026#39;t complain. 7 // converge this temp into s.postLikes 8 s.mu.Lock() 9 defer s.mu.Unlock() 10 for key, value := range temp { 11 for _, user := range value.GetSet() { 12 _ = s.postLikes.AddLike(key, user) 13 } 14 } 15} Also we want to be notified when some node joins, leaves or update the cluster which we subscribe by implementing the membership.EventDelegate interface. The implementation of the interface looks like-\n1func (s *server) NotifyJoin(n *memberlist.Node) { 2 s.logger.Info().Msgf(\u0026#34;node joined: %s address: %s \\n\u0026#34;, n.String(), n.Addr.String()) 3} 4 5func (s *server) NotifyLeave(n *memberlist.Node) { 6 s.logger.Info().Msgf(\u0026#34;node left: %s \\n\u0026#34;, n.String()) 7} 8 9func (s *server) NotifyUpdate(n *memberlist.Node) { 10 s.logger.Info().Msgf(\u0026#34;node updated: %s \\n\u0026#34;, n.String()) 11} Also we need to broadcast the likes we get on a particular node to all the nodes for which we use gossip protocol of the membership layer by implementing the membership.Broadcast interface. The gossip protocol works exctly like how gossip works in general. You tell your friends something they tell something to their friends and so on the chain goes on and eventually the gossip is spread out to a whole lot of people. Similarly you get an event you gossip it to some nodes, which gossip it to some nodes again and so on, and so the event reaches every node eventually. So, on receiving the post request for like we emit an event in broadcast queue to be broadcasted to the cluster.\nAnd the get request gets the information from it\u0026rsquo;s local in memory database, and therefore it might be possible the like recorded at a particular node might not have reached here until now, but that\u0026rsquo;s alright we can be eventually consistent here. We can show the like after sometime, the important thing is that we don\u0026rsquo;t lose the like. If you want stronger consistency for example- linearizability- you might want something like writing to a quorum of nodes at the time of writing a like and then getting it from the quorum of nodes again while reading the likes for a post. That\u0026rsquo;s it, that\u0026rsquo;s the distributed service/counter. Now we need some sort of orchestration to deploy these services we are going to be using kubernetes for that. Not pasting the entire yamls just pasting some relevant code.\nThe membership requires atleast one known node in the cluster to join the cluster. so we are going to create a master deployment with\n1labels: 2 workertype: master 3 application: likes-distribute and a master deployment service to be able to connect with the pods described by master deployment.\n1spec: 2 selector: 3 workertype: master 4 application: likes-distributed This can be a clusterip service because only the worker nodes that we are going to create just right after, are going to connect to master via this service. The worker nodes are not actually workers and the request can land up on any of the nodes/pods but the membership requires atleast one known member of the cluster to join the cluster.\n1metadata: 2 labels: 3 workertype: worker 4 application: likes-distributed Distinguishing from master so that every pod doesn\u0026rsquo;t create it\u0026rsquo;s own cluster, then that\u0026rsquo;s just meaningless. Now we need some service to be able to connect to n workers and 1 master.\n1spec: 2 selector: 3 application: likes-distributed 4 type: LoadBalancer 5 ports: 6 - port: 80 7 nodePort: 31001 8 protocol: TCP 9 targetPort: 8080 10 name: http-server You can check out the whole source code here , if you like the repo don\u0026rsquo;t forget to give it a start. You can use make run to run the kubernetes cluster on your local, if you have minikube installed. It uses configmaps as well so please apply in order if you are applying manually. Then use-\n1minikube service distributed-likes-service-cluster This will give you the service endpoint which you can use to register likes or get likes etc. A sample request looks something like-\nSo, as you can see the request landed on some worker pod but quickly got replicated to all the pods which can be seen in subsequent curl requests.\nThere\u0026rsquo;s a lot of scope of improvement in this service which is going to be iterative and many more blogs might come up with the improvements. Thanks.\n","date":"Sep 1, 2021","img":"","permalink":"/posts/distributed-counter/","series":["Distributed Systems"],"tags":["CRDTs"],"title":"Distributed Counter"},{"categories":null,"content":"Vishal Sharma(he/him/they) is a software developer from India. He started his software developer journey professionally in 2020. He works majorly on backend writing softwares/services that can scale and are inherently distributed. He writes mostly is mostly in go.\n","date":"Feb 28, 2019","img":"","permalink":"/about/","series":null,"tags":null,"title":"About"}]